% math.hm
% mathematics!

\setoutputdir{math}
\addbreadcrumb{page_math}

\begin{page}{math.html}
  \label{page_math}
  \title{Mathematics}
  \modified{3 June 2012}

  This is an index of writing on mathematics.

  \begin{itemize}
  \item \ref{page_math_fib_diffeq}
  \item \ref{page_math_platonic}
  \item \ref{page_math_recurrence}
  \item \ref{page_math_relations}
  \item \ref{page_math_intervals}
  \item \ref{page_math_roots_unity}
  %\item \ref{page_quotient_groups}
  \end{itemize}

  At some point, I hope to put up my lecture notes for a
  Splash\footnote{This is a program put on by ESP at MIT every year
    where hundreds of high-school (and younger) students come for a
    weekend to be taught about virtually any topic.} class I taught in
  2009 called ``The Joy of Eigenvalues'':
  \begin{quote}
    Linear recurrence relations, generating functions, graphs, oh my!
    We will explore the connection between these seemingly unrelated
    ideas by the numbers which are intrinsic to their
    structure. Applications include Google PageRank, vibrating
    systems, and population dynamics. We will start with the
    definition of an eigenvalue and continue from there (with many
    examples and $\lambda$'s).
  \end{quote}

  \section{Small problems}

  These are just some small problems with their solutions.

  \hline

  This one was given to me in my 5th grade math class.

  \textit{Suppose you have an array of lockers, labeled $1$, $2$, $3$,
    and so on, and they're all closed.  By ``toggling a locker,'' we
    mean opening it if its closed and closing it if it is open. You
    have a lot of time on your hands, so you decide to do the
    following.  First you toggle every locker, then you toggle lockers
    $2$, $4$, $6$, and so on (multiples of two).  Then you toggle
    lockers which are multiples of three, and then you keep doing this
    process of toggling the multiples of four, five, six, and so on.
    Which lockers are open by the end, if any? }

  You can try this experimentally, and with about 20 lockers you can
  see the pattern: the lockers which are open are those which are
  squares! (i.e., $1, 4, 9, 16, \ldots$).

  Why is this?  First, to determine whether a particular locker is
  open or not, all you need to know is whether the number of times it
  was toggled is even or odd.  Each locker is toggled once for each of
  its divisors, and divisors usually come in pairs (if $d$ is a
  divisor of $n$, then $n/d$ is a divisor of $n$, too).  But are $d$
  and $n/d$ distinct?  If $n$ is a square, then $d$ being its square
  root gives $d=n/d$.  If $n$ is not a square, then $d$ and $n/d$ are
  always different.  This means that a square has an odd number of
  divisors, and a non-square has an even number of divisors; hence,
  the squares are the ones with the open lockers!

\end{page}

\begin{page}{platonic.html}
  \label{page_math_platonic}
  \title{The five Platonic solids}
  \modified{3 June 2012}

  \begin{figure}[r]
    \label{fig_platonic}
    \includegraphics[alt=The five Platonic solids,width=300]{platonic_solids.jpg}
    \begin{center}
      \caption{Paper models of all five Platonic solids.}
    \end{center}
  \end{figure}

  I've been fascinated by polyhedra since I was really young.  My mom
  had this book called \link[\textit{Polyhedron Models for the
    Classroom}]{http://www.amazon.com/Polyhedron-Models-Classroom-Magnus-Wenninger/dp/0873530837}
  by Magnus Wenniger\footnote{According to a review of
    \textit{Polyhedron Models for the Classroom} on Amazon, the book
    is an excerpt from the book \link[\textit{Polyhedron
      Models}]{http://www.amazon.com/Polyhedron-Models-Magnus-J-Wenninger/dp/0521098599}
    by the same author.} which gave patterns for building models of
  all the regular and semiregular polyhedra as well as a short history
  of the pursuit to discover all of them.  I thought it was really
  cool, and I've made many models of the Platonic and Archimedean
  solids through the years.  Unlike polygons, there are only
  \textit{finitely many} Platonic solids.  Why is that?  In this page,
  I'll give a proof that there are at most five Platonic solids.

  The picture to the right shows a set of models of all five Platonic
  solids.  From left to right they are the tetrahedron, the
  dodecahedron, the cube (or hexahedron), the icosahedron, and the
  octahedron, and they are each named for their respective number of
  faces.  These forms have been known for thousands of years, and were
  named after Plato who, for better or for worse, said that each
  polyhedron corresponded to a different classical element (earth,
  air, fire, and water; he left the dodecahedron to the gods).  They
  really should have been named after Theaetetus, a contemporary of
  Plato, who first proved that there were exactly five regular convex
  polyhedra.  Or even Euclid, who completed the \textit{Elements} with
  constructions of each of the solids.

  I haven't been clear about what I mean by a regular polyhedron, so I
  give the following definition:

  \textbf{Definition.} A \emph{regular polyhedron} is a polyhedron
  whose faces are congruent regular polygons such that each vertex has
  the same number of incident faces.

  For a given regular convex polyhedron, we will let $n$ be the number
  of sides of each of its polygonal faces and $\mu$ be the number of
  faces around each vertex (where $\mu$ stands for
  \textit{monodromy}).  Out of our familiarity with regular polygons,
  we immediately know that $n\geq 3$.  I'll appeal to our geometric
  intuition to see that $\mu\geq 3$ as well (polyhedra must have some
  actual volume, which is analogous to the bound on $n$ for polygons).

  Let $v$, $e$, and $f$ be the numbers of vertices, edges, and faces,
  respectively of the polyhedron.  The quantity $\chi=v-e+f$ is known
  as the \textit{Euler characteristic}, and for convex polyhedra,
  $\chi$ is always $2$, a fact which has been proven so many times I'm
  not going to show it here.\footnote{ See \link[Nineteen Proofs of
    Euler's
    Theorem]{http://www.ics.uci.edu/~eppstein/junkyard/euler/}.

    One method of proof which isn't listed uses algebraic
    topology. First you show you can replace each face with triangles,
    and this does not change $\chi$.  This gives a simplicial complex
    which is homeomorphic to a $2$-sphere.  There is an exercise of
    Hatcher which says that alternating sums of the degrees of the
    homology groups of a space are constant up to homotopy
    equivalence.  For a $2$-sphere, this alternating sum is $1-0+1=2$.
    Since for simplicial complexes the alternating sum is the Euler
    characteristic, this shows that $\chi=2$.}

  We now make two observations.  The first is that $nf=2e$, which is
  obtained by noticing that each face in the polyhedron has $n$ edges
  around it, but each edge has two incident faces, so $nf$ double
  counts the number of edges.  The second is that $\mu v=2e$, which is
  obtained by noticing that each vertex has $\mu$ incident edges
  (separating each of the $\mu$ faces), and each edge has two incident
  vertices, so $\mu v$ also double counts the number of edges.  With
  these, we can write $v-e+f=2$ in terms of $n$, $\mu$, and $e$ as
  follows:
  \begin{equation*}
    \frac{1}{n} + \frac{1}{\mu} = \frac{1}{2} + \frac{1}{e}
  \end{equation*}
  I'm told that diophantine equations which are sums of reciprocals
  involve deep number theory, which is something I'd like to
  understand.\footnote{The page \link[Egyptian
    Fractions]{http://mathworld.wolfram.com/EgyptianFraction.html} on
    MathWorld deals with some equations of this form.  I think it also
    has something to do with Lie something-or-another.}
  
  Since $e$ is positive, this equation reduces to the following
  inequality:
  \begin{equation*}
    \frac{1}{n} + \frac{1}{\mu} > \frac{1}{2}
  \end{equation*}
  Let's now just do a case analysis to find all the solutions to this
  inequality.  We'll go through each possible value of $n$, starting
  with $3$ since $n\geq 3$.  For the following, recall that $\mu\geq
  3$.
  \begin{description}
  \item[\textbf{Case I.} $n=3$] In this case, $\mu<6$.  When $n=3$ and
    $\mu=3$, this is a tetrahedron, when $n=3$ and $\mu=4$, this is an
    octahedron, and when $n=3$ and $\mu=5$, this is an icosahedron.
  \item[\textbf{Case II.} $n=4$] In this case, $\mu<4$, so the only
    solution is $n=4$ and $\mu=3$, which is a hexahedron (cube).
  \item[\textbf{Case III.} $n=5$] In this case, $\mu<10/3$, so we have
    $n=5$ and $\mu=3$, which is the dodecahedron.
  \item[\textbf{Case IV.} $n\geq 6$] Then, since $\mu\geq 3$,
    \begin{equation*}
      \frac{1}{n} + \frac{1}{\mu} \leq \frac{1}{6} + \frac{1}{3} = \frac{1}{2},
    \end{equation*}
    so there is no solution.
  \end{description}

  We can verify that each of these solutions have corresponding
  integral values for $v$, $e$, and $f$, which we compute in the
  following table:
  \begin{center}
  \begin{tabular}{rr|rrr|l}
    $n$ & $\mu$ & $v$ & $e$ & $f$ & name \\
    \hline
    \hline
    3 & 3  & 4 & 6 & 4 & tetrahedron \\
    3 & 4  & 6 & 12 & 8 & octahedron \\
    3 & 5  & 12 & 30 & 20 & icosahedron \\
    4 & 3  & 8 & 12 & 6 & hexahedron (cube) \\
    5 & 3  & 20 & 30 & 12 & dodecahedron \\
  \end{tabular}
  \end{center}

  Therefore, we have shown that there cannot be more than five
  Platonic solids!

  Unfortunately, we have only proven that there are \textit{at most}
  five regular convex polyhedra, and no attempt has been made to show
  that each of these solutions correspond to an actual regular
  polyhedron, despite the suggestive images of paper models and my
  attempts identify the solutions with them.  I'll leave this as an
  exercise for the reader.  Or, you can read Book XIII of the
  \textit{Elements} (and probably most of the books leading up to it)
  to get constructions of each of them.
  
\end{page}

\begin{page}{quot_groups.html}
  \label{page_quotient_groups}
  \title{An exposition of quotient groups}
  \modified{2 December 2012}

  The purpose of this page is to show a different way of looking at
  quotient groups (and quotients in general).  A common presentation
  for quotient groups is to take the set of cosets and show that there
  is a natural group operation defined on them.  However, I feel that
  this muddles the intuition for the construction.

  Let $G$ be some arbitrary group, and let $H$ be some subgroup of
  $G$.  We can define a relation on $G$ as follows:
  \begin{equation*}
    x\equiv y\pmod{H}\text{\ if\ and\ only\ if\ }xy\inv\in H.
  \end{equation*}
  Note that the $\pmod{H}$ refers to the $\equiv$ symbol, not $y$.  We
  pronounce this as ``$x$ is equivalent to $y$, modulo $H$''.

  If we consider $G=\Z$ as an additive group, and let $H$ be the set
  of multiples of $5$, then this relation amounts to the standard
  definition for ``modulo 5'' on the integers (i.e., $x\equiv
  y\pmod{5}$ if and only if $x$ and $y$ are different by a multiple of
  $5$).

  The first minor result is that this relation is actually an
  equivalence relation (hence the suggestive pronunciation of the
  symbol).  Reflexivity follows from the definition of inverses,
  symmetry follows from the fact that the subgroup $H$ is closed under
  inverses, and transitivity follows from $H$ being closed under
  composition: if $x\equiv y$ and $y\equiv z\pmod{H}$, then both
  $xy\inv$ and $yz\inv$ are in $H$, so $(xy\inv)(yz\inv)$ is in $H$,
  too, and therefore $xz\inv$ is in $H$.

  The second minor result is that the composition law respects the
  equivalence, if and only if $H$ is a normal subgroup: that is, if
  $x_1\equiv x_2$ and $y_1\equiv y_2\pmod{H}$, then $x_1y_1\equiv
  x_2y_2\pmod{H}$.  Intuitively, this means we can ``replace likes
  with likes.''  Why does this work?  First, we know that $y_1y_2\inv$
  is in $H$.  Since $H$ is normal, this means that $x_1y_1y_2\inv
  x_1\inv$ is in $H$, too.  Since $x_1x_2\inv$ is in $H$, we can
  compose these to get $x_1y_1y_2\inv x_2\inv\in H$, which is the same
  as $x_1y_1(x_2 y_2)\inv$, and this proves that $x_1y_1\equiv
  x_2y_2\pmod{H}$.  Conversely, supposing that we can replace likes
  with likes, we will show that $H$ must therefore be normal.  Let
  $y\in H$ and $x\in G$.  Since $y\equiv 1\pmod{H}$ we have that
  $xyx\inv\equiv 1\pmod{H}$, by first left-multiplying and then
  right-multiplying by $x$ and $x\inv$, respectively.  Since this is
  equivalent to saying that $xyx\inv\in H$, and since both $x$ and $y$
  were arbitrary, this proves $H$ is a normal subgroup of $G$.

  The third minor result is that inverses still work as expected, but
  this is really easy to show: 

\end{page}

\begin{page}{relations.html}
  \label{page_math_relations}
  \title{The theory of relations}
  \modified{30 November 2012}

  I was reading \textit{Naive Set Theory} by Halmos, and he mentioned
  a reasonable definition for the composition of two relations, as
  well as the characterization of an equivalence relation using
  relation composition.  On this page, I want to explore relations in
  some more detail (this is presently a work in progress).

  We define a relation $R$ between two sets $X$ and $Y$ to be any
  subset of $X\times Y$, which we write as $R:X\leftrightarrows Y$ for
  short.  If $(x,y)\in R$, then we say that $x$ is related to $y$ by
  $R$.  We will write this symbolically as $y\r{R}x$.  While the
  standard notation for this is $x\r{R}y$, we differ here because it
  will aid in the notation for relation composition and for functions.

  We write $R(x)$ for the set of all $y$ such that $y\r{R}x$.  Or, if
  $U$ is a subset of $X$, then we write $R(U)$ to be the union of all
  the $R(x)$, where $x\in U$.  We define $(y)R$ and $(V)R$
  analogously.  If it so happens that $R(x)$ or $(y)R$ are singleton
  sets, then we pretend that $R(x)$ or $(y)R$ are the element of their
  corresponding singleton set.  Two special sets are the
  \textit{image} of a relation, $R(X)$, and the \textit{co-image} (or
  \textit{domain}) of a relation, $(Y)R$.

  The inverse of a relation $R:X\leftrightarrows Y$ is a relation
  $R\inv:Y\leftrightarrows X$ defined by $x\r{R\inv}y$ whenever
  $y\r{R}x$.  Generally, something is \textit{co-} for some relation
  if the non-\textit{co-} version applies to its inverse (i.e., ``the
  arrows are reversed'').

  There are a few important properties for a relation:
  \begin{itemize}
  \item $R$ is a \textit{surjection} if for any $y\in Y$, there exists
    an $x\in X$ such that $y\r{R}x$.  We can also define surjectivity
    by the condition $R(X)=Y$.
  \item $R$ is a \textit{co-surjection} if $R\inv$ is a surjection.
    Similarly, we can also say that $R$ is co-surjective if $(Y)R=X$.
  \item $R$ is an \textit{injection} if whenever $x_1,x_2\in X$ and
    $y\in Y$ are such that $y\r{R}x_1$ and $y\r{R}x_2$, then
    $x_1=x_2$.  We could just have well said that $R$ is
    \textit{one-to-many}.  An example of such a relation is ``is the
    biological mother of'' if $X$ and $Y$ are both the set of all
    people.
  \item $R$ is a \textit{co-injection} if $R\inv$ is an injection.
    This property is the same as saying that $R$ is
    \textit{many-to-one}.  An example of this is the relation ``lives
    in'' for $X$ being the set of all people and $Y$ being the set of
    all houses.  Note that this relation isn't necessarily a
    co-surjection since not everyone needs to live in a house.
  \end{itemize}

  A relation which is both an injection and an co-injection can also
  be called \textit{one-to-one}.  An example of such a relation is
  ``is the right foot of'' where $X$ is the set of feet and $Y$ is the
  set of people.

  A \textit{function} $f:X\to Y$ is a relation which is a
  co-surjection and a co-injection.  This means that for every $x\in
  X$, there is exactly one $y\in Y$ such that $y\r{f}x$.  We refer to
  this $y$ using the notation $f(x)$.  A \textit{co-function} is a
  relation whose inverse is a function.  Note that this means a
  co-function is a surjection and an injection.  Hence, a
  \textit{bijection} is a relation which is both a function and a
  co-function.

  We define the composition of two relations $R:X\leftrightarrows Y$
  and $S:Y\leftrightarrows Z$ to be a relation $SR:X\leftrightarrows
  Z$ defined by $z\r{SR}x$ whenever $z\r{S}y$ and $y\r{R}x$ for some
  $y\in Y$.  In general, if $R$ and $S$ are relations which both
  satisfy the same one of the four properties, then $SR$ has that
  property, too.  In particular, if $f:X\to Y$ and $g:Y\to Z$ are
  functions, then we see that $gf$ is also a function: $gf$ is a
  co-surjection since, for any $x\in X$, there is a $y\in Y$ such that
  $y\r{f}x$, and there is a $z\in Z$ such that $z\r{g}y$, hence
  $z\r{gf}x$; and $gf$ is a co-injection since, if $z_1,z_2\in Z$ and
  $x\in X$ are such that $z_1\r{gf}x$ and $z_2\r{gf}x$, there are
  $y_1,y_2\in Y$ such that $y_1\r{f}x$ and $y_2\r{f}x$, so $y_1=y_2$
  by co-injectivity of $f$, and hence $z_1=z_2$ by co-injectivity of
  $g$.

  What is the inverse of the composition of relations?  Suppose we
  want to check if $x\r{(SR)\inv} z$, which is equivalent to
  $z\r{SR}x$, which is equivalent to some $y\in Y$ existing such that
  $z\r{S}y$ and $y\r{R}x$, which is equivalent to $x\r{R\inv} y$ and
  $y\r{S\inv} x$ for some $y\in Y$, and this is equivalent to
  $x\r{R\inv S\inv} y$.  Therefore, $(SR)\inv=R\inv S\inv$.

  One can see that composition of relations is associative, as one
  would expect (at least one would expect for functions, at least).

  Define $I_A:A\leftrightarrows A$ to be the relation consisting only
  of $(a,a)$ for all $a\in A$.  Some common properties for a relation
  $R:X\to X$ can be described as follows:
  \begin{itemize}
  \item A relation is \textit{reflexive} if $I_X\subset R$.
  \item A relation is \textit{symmetric} if $R\subset R\inv$ (or,
    equivalently, if $R\inv\subset R$).
  \item A relation is \textit{transitive} if $RR\subset R$.
  \end{itemize}

  An \textit{equivalence relation} on $X$ is a relation
  $R:X\leftrightarrows X$ which satisfies all three of these
  properties (which I suppose can be succinctly written as $I_X\subset
  RR\subset R\subset R\inv$, using the fact that $R\subset RR$ by the
  reflexive property).

  There is a canonical equivalence relation on $X$ for a function
  $f:X\to Y$ defined by $f\inv f$, which says that two elements $x_1$
  and $x_2$ in $X$ are related if and only if $f(x_1)=f(x_2)$.  The
  proof for this is straight-forward.  First, for an arbitrary $x\in
  X$, there is a $y\in Y$ such that $y\r{f}x$, so since $x\r{f\inv}
  x$, we have reflexivity: $x\r{f\inv f}x$.  Second suppose that $x_2
  \r{f\inv f} x_1$.  Then there is a $y\in Y$ such that $y \r{f} x_2$
  and $y \r{f} x_1$, so $x_1\r{f}x_2$, hence symmetry.  Third, suppose
  that $x_3 \r{f\inv f} x_2$ and $x_2 \r{f\inv f} x_1$.  Then there
  are $y$ and $z$ such that $y \r{f} x_3$, $y \r{f} x_2$, $z \r{f}
  x_2$, and $z \r{f} x_1$.  By co-injectivity, $y=z=f(x_2)$, and so
  $x_3 \r{f\inv f} x_1$, hence transitivity.

  As an aside, since an equivalence relation is equivalent to a
  partition of a set, we have a first isomorphism theorem for sets by
  this construction: the set of partitions of $X$ induced by $f\inv f$
  is in bijective correspondence to the image of $f$.

  Suppose $R:X\leftrightarrows X$ is a relation on a set $X$.  What is
  the smallest equivalence relation $E$ on $X$ which contains $R$?  We
  know that $I_X$ must be in $E$ for reflexivity, and we know that
  $R\inv$ must be in $E$, too. Let $S=R\cup R\inv$, which is the
  symmetrified $R$.  Let $E$ be the union of $I_X$, $S$, $SS$, $SSS$,
  $SSSS$, and so on (this could be called the \textit{transitive
    closure} of $S$ if it didn't have the $I_X$ term).  For
  convenience, let $S^n$ represent $S$ composed with itself $n$ times,
  with $S^0=I_X$.  Since each $S^n$ is symmetric, $E$ is symmetric as
  well.  Furthermore, suppose $y\r{E}x$ and $z\r{E}y$.  Then,
  $y\r{S^n}x$ and $z\r{S^m}y$ for some $n,m$.  Then, $z\r{S^{n+m}}y$,
  and so $E$ is transitive.  %Actually, we could have just let $E=H\inv
%  H$, where $H=I_X\cup R$.  Note the similarity between the formulas
%   \begin{equation*}
%     \frac{1}{1+x} = 1 + x + x^2 +x^3+\ldots
%   \end{equation*}
%   and
%   \begin{equation*}
%     (I_X\cup R)\inv = I_x\cup R\cup R^2\cup R^3\cup\ldots.
%   \end{equation*}
%   There are nice algebraic properties between unions and compositions
%   of relations.  For instance, if $T,U,V:X\leftrightarrows Y$ are
%   relations, $(T\cup U)V=TV\cup UV$.  I'm sure more can be said about
%   this aspect of relations, but I'll leave that as an exercise for the
%   reader until I get around to it.
  This shows $E$ is an equivalence relation containing $R$, but is it
  the smallest such?  Since any equivalence relation which contains
  $R$ must contain $I_X$, $S$, $SS$, and so on, and $E$ contains no
  more than this, it follows that $E$ is smallest.

  It turns out we can define (co-)surjectivity and (co-)injectivity
  using composition properties, too:
  \begin{itemize}
  \item A relation is surjective if and only if $I_Y\subset RR\inv$.
  \item A relation is co-surjective if and only if $I_X\subset R\inv R$.
  \item A relation is injective if and only if $R\inv R\subset I_X$.
  \item A relation is co-injective if and only if $RR\inv\subset I_Y$.
  \end{itemize}

  So, if $R\inv R=I_X$, then $R$ is co-surjective and injective, and
  if $RR\inv=I_Y$, then $R$ is surjective and co-injective.  So, if
  $S:X\leftrightarrows X$, and $S\inv S=SS\inv=I_X$, then $S$ is a
  bijection (and, as a consequence, a group of relations with relation
  composition as the binary operation is actually a group of
  bijections).

  We see that some of the proofs for $f\inv f$ being an equivalence
  relation can be done more algebraically: first, $(f\inv f)\inv=f\inv
  f$, so it is symmetric; second, $(f\inv f)(f\inv
  f)=f\inv(ff\inv)f=f\inv I_Af$ since $f$ is co-injective, where
  $A\subset Y$ is the image of $f$, and $f\inv I_Af=f\inv f$, hence
  $f\inv f$ is transitive.

  There are nice algebraic properties for unions and intersections.
  Let $R:X\leftrightarrows Y$ and $U,V\subset X$.  Then $R(U\cup
  V)=R(U)\cup R(V)$, by definition.  We also have that $R(U\cap
  V)\subset R(U)\cap R(V)$, since if $y\r{R}x$ for some $x\in U\cap
  V$, then because $x$ is in both $U$ and $V$, $y$ is in $R(U)\cap
  R(V)$.  If, in addition, $R$ is surjective, then it is an equality:
  $R(U\cap V)=R(U)\cap R(V)$.  The inverse of a function is a
  surjective relation, so this is why unions and intersections work so
  nicely with them.
  
\end{page}

\begin{page}{recurrence.html}
  \label{page_math_recurrence}
  \title{Multivariable recurrence relations}
  \modified{9 May 2013}

  I'm going to ramble on about what I found out about multivariable
  recurrence relations today, and digress here and there.  One thing
  that I found to be particularly interesting was a way to show that
  Pascal's triangle has binomial coefficients as entries.

  \section{An example of methods}
  \label{sec:an-example-methods}

  The following is from a 6.006 problem set that Colleen showed me
  (which I paraphrase, keeping some of the silly flavor of the
  original).
  \begin{quote}
    Right when an upcoming concert is announced, some extraordinary
    fans of the band dutifully line up in front of the ticket office.
    When it's time, the ticket master opens his door and says, ``Hey!
    So you think you're all fans? I'll show you what a \textit{real}
    fan is!'' and then points a large fan at the them, turning it on.
    Each person is either sitting in a flimsy chair or standing; the
    fan blows away those that are sitting in a chair and those that
    are standing in front of a sitting person, but the line is so long
    that the last person in line is standing in front of a brick wall
    and so is not blown away.  How many ways can $n$ fans be sitting
    or standing so that exactly $k$ fans remain?  (and write a program
    to compute this.)
  \end{quote}
  Let's just go through with finding the recurrence relation --- it
  doesn't take too long.  We define $T_u(n, k)$ to be the number of
  configurations of $n$ fans so that $k$ remain, given that, behind
  the last fan (i.e., after these $n$ fans), there is either a
  standing fan or a brick wall; the idea is that we know from the
  given that if the last fan is standing, that fan will remain.  Let
  $T_d(n, k)$ be the same, except it supposes that the last person is
  in front of a fan who's sitting, so, sitting or standing, there is
  no way the last fan can remain.  From these, we get the following
  two equations, where the left-hand side of the addition corresponds
  to the last person in line standing, and the right-hand side,
  sitting:
  \begin{equation*}
    T_u(n, k) = T_u(n-1,k-1) + T_d(n-1,k) 
  \end{equation*}
  \begin{equation*}
    T_d(n, k) = T_u(n-1,k) + T_d(n-1,k)
  \end{equation*}
  The most remarkable term is $T_u(n-1,k-1)$: the $k-1$ is there
  because we assume the last person is standing, so they know they
  will remain; hence, one fewer required fan in the remainder of the
  line.

  These are sufficient for writing a $O(nk)$-time algorithm to compute
  $T_u(n, k)$.\footnote{ For completeness, here's a short Python
    program to compute $T(n,k)$ in $O(nk)$ time.  This performs some
    extraneous computations out of simplicity, but we're not trying to
    break a speed record here.
\begin{verbatim}
def T(n, k) :
    if n < 0 or k < 0 :
        return 0
    valsu = [0, 1] + [0]*k
    valsd = [0, 1] + [0]*k
    for i in xrange(1, n+1) :
        valsu2 = [0]
        valsd2 = [0]
        for j in xrange(0, k + 1) :
            valsu2.append(valsu[j] + valsd[j+1])
            valsd2.append(valsu[j+1] + valsd[j+1])
        valsu = valsu2
        valsd = valsd2
    return valsu[k+1]
\end{verbatim}
  } But, I'm more interested right now in understanding these
  mutually-recursive multivariable functions.

  From the second, we see that $T_d(n-1,k) = T_d(n,k)-T_u(n-1,k)$,
  and, combining this with the first, we get $T_d$ in terms of $T_u$:
  \begin{equation*}
    T_d(n,k)=T_u(n,k)+T_u(n-1,k)-T_u(n-1,k-1),
  \end{equation*}
  which we can put back into the first to get the following recurrence
  relation:
  \begin{equation*}
    T_u(n, k) = T_u(n-1,k-1) + T_u(n-1,k) + T_u(n-2,k)-T_u(n-2,k-1)
  \end{equation*}
  From now on, we'll write $T=T_u$.

  Just to get a sense of the function, here is a table of $T(n,k)$
  with the the rows and columns being $n$ and $k$.\footnote{Like
    Pascal's triangle, the data in each diagonal parallel to the major
    diagonal is described by a polynomial.  I couldn't find any
    reasonable pattern in these polynomials, though.}
  \begin{center}
    \begin{tabular}{rrrrrrrrrl}
      1 &  &  &  &  &  &  &  &  \\
      1 & 1 &  &  &  &  &  &  &  \\
      2 & 1 & 1 &  &  &  &  &  &  \\
      3 & 3 & 1 & 1 &  &  &  &  &  \\
      5 & 5 & 4 & 1 & 1 &  &  &  &  \\
      8 & 10 & 7 & 5 & 1 & 1 &  &  &  \\
      13 & 18 & 16 & 9 & 6 & 1 & 1 &  &  \\
      21 & 33 & 31 & 23 & 11 & 7 & 1 & 1 &  \\
      34 & 59 & 62 & 47 & 31 & 13 & 8 & 1 & 1 \\
      \vdots & & & & & & & & & \ddots
    \end{tabular}
  \end{center}

  My motivating question is whether there's a closed-form solution to
  compute $T$. While I didn't actually find one, I learned about
  multivariable generating functions in the process, and this is
  mainly what I'm wanting to write about.  The multivariable case of a
  generating function is similar to the single variable case, except
  that there is a $c_{ij}x^iy^j$ for every term $c_{ij}$ (in what
  might be called a bi-sequence).  Hence, we let
  \begin{equation*}
    g(x,y)=\Sigma_{i,j}T(i,j)x^iy^j
  \end{equation*}
  be our generating function.

  I managed to compute $g$ at first as a generating function of
  generating functions, fixing each variable one at a time, using
  Mathematica to do some of the hairy symbolic manipulation.  However,
  it turns out there is a much simpler way.  To see how we'd do this,
  consider that
  \begin{equation*}
    x^ay^bg(x,y) = \Sigma_{i,j}T(i,j)x^{i+a}y^{j+a} = \Sigma_{i,j}T(i-a,j-b)x^iy^j
  \end{equation*}
  by defining $T$ of negative numbers to be zero.  That is,
  multiplication of $g$ by $x^ay^b$ gives a generating function $g'$
  of
  \begin{equation*}
    T'(n,k)=T(n-a,k-b). 
  \end{equation*}
  Hence, the right hand side of the recurrence relation has the
  generating function
  \begin{equation*}
    xyg(x,y)+xg(x,y)+x^2g(x,y)-x^2yg(x,y),
  \end{equation*}
  and when we consider the boundary conditions (namely that
  $T(0,0)=1$), we have
  \begin{equation*}
    g(x,y)=1+    xyg(x,y)+xg(x,y)+x^2g(x,y)-x^2yg(x,y),
  \end{equation*}
  and so
  \begin{equation*}
    g(x,y)=\frac{1}{1-x-x^2-xy+x^2y}.
  \end{equation*}
  Aside: if $y=0$ (which is equivalent to saying $k=0$ since $y=0$
  kills all terms involving non-zero $k$), this reduces to the
  recurrence relation of the Fibonacci sequence.  In fact, a
  combinatorial proof is that $k=0$ corresponds to having each person
  stand only when there is a sitting person behind them; that is, tile
  an $n\times 1$ board with $2\times 1$ and $1\times 1$ pieces,
  representing a stander-sitter and a sitter, respectively.  I've seen
  that this problem is counted by the Fibonacci sequence, so it's
  enough of a proof for me at least!

  Unfortunately, this polynomial is irreducible, so there's not really
  anything I know we can do to find a closed form.  Perhaps
  interestingly, using the geometric series
  $1/(1-t)=1+t^2+t^3+\ldots$, we can rewrite $g$ in the form
  \begin{equation*}
    g(x,y)=1+(x+x^2+xy-x^2y) + (x+x^2+xy-x^2y)^2+(x+x^2+xy-x^2y)^3+\ldots,
  \end{equation*}
  so to compute $T(n,k)$, we need only keep track of which of these
  terms contribute to $x^ny^k$, which can only happen for exponents
  (roughly) from $(n+k)/3$ to $n+k$.

  \section{The binomial coefficient}
  \label{sec:another-function}

  Another function which is conducive to study using multivariable
  recurrences is the binomial coefficient. Let's say we start with
  Pascal's triangle:
  \begin{center}
    \begin{tabular}{rrrrrl}
      1 \\
      1 & 1\\
      1 & 2 & 1 \\
      1 & 3 & 3 & 1 \\
      1 & 4 & 6 & 4 & 1 \\
      \vdots & &  & &  & \ddots
    \end{tabular}
  \end{center}
  which is generated by the recurrence $P(i,j)=P(i-1,j)+P(i-1,j-1)$,
  where $P$ is $0$ everywhere outside the triangle, and $P(0,0)=1$,
  which is the tip of the triangle.  Using similar techniques, and
  letting $f(x,y)$ be the generating function of $P$, we have
  \begin{equation*}
    f(x,y)=1+xf(x,y)+xyf(x,y),
  \end{equation*}
  and the closed form for the generating function is
  \begin{equation*}
    f(x,y)=\frac{1}{1-x-xy}.
  \end{equation*}
  Similarly rewriting using the geometric series, we have
  \begin{equation*}
    f(x,y)=1+(x+xy)+(x+xy)^2+(x+xy)^3+\ldots.
  \end{equation*}
  Importantly, each monomial in $f$ comes from exactly one of the
  $(x+xy)^n$ terms, which are each composed of monomials $x^ny^{n-k}$
  (and $x^n$ can only come from this particular term).  What are the
  coefficients of these monomials?  They're the binomial coefficients
  of course!\footnote{Where we're defining binomial coefficients to
    the the coefficients of polynomials $(x+y)^n$, for varying $n$.
    The ``of course'' refers to how they are binomial coefficients by
    definition.}  Therefore, the entries in Pascal's triangle are all
  binomial coefficients!

  The identity involving binomial coefficients (which we'll denote by
  $B$), namely
  \begin{equation*}
    B(n, k) = B(n-1,k) + B(n-1,k-1),    
  \end{equation*}
  follows from the recurrence relation.

  What about going the other way?  These $x+xy$ terms aren't as nice
  as $x+y$, and $(x+y)^n$ gives binomial coeffients just as well.  So,
  consider the generating function
  \begin{equation*}
    h(x,y)=1+(x+y)+(x+y)^2+\ldots=\frac{1}{1-x-y},
  \end{equation*}
  which has $h(x,y)-xh(x,y)-yh(x,y)=1$ and a corresponding recurrence
  relation
  \begin{equation*}
    H(a,b)=H(a-1,b)+H(a,b-1)
  \end{equation*}
  with $H(0,0)=1$.  This is actually Pascal's triangle again, except
  transformed into a right triangle.

  This suggests a notation I sometimes would rather have for the
  Binomial coefficient:\footnote{I haven't seen this notation anywhere
    else; I'd definitely like to see where others have used similar
    notation if it's been done.} We define $[a,b]$ to be the number of
  ways of ordering $a$ of one kind of indistinguishable object among
  $b$ of another (that is, it's $(a+b)$ choose $a$).  With this
  notation, the Binomial theorem would look like
  \begin{equation*}
    (x+y)^n=\Sigma_{a+b=n}[a,b]x^ay^b.
  \end{equation*}
  The recurrence relation provides the following identity:
  \begin{equation*}
    [a,b] = [a-1,b] + [a,b-1].
  \end{equation*}
  What is the intuition for this?  The first object in any permutation
  is either an object of the first kind or of the second kind; add up
  each case.

  Using similar combinatorial reasoning, the notation can be extended
  to a multinomial coefficient with a similar identity:
  \begin{equation*}
    [a_1,a_2,\ldots,a_n] = [a_1-1,a_2,\ldots,a_n] + [a_1,a_2-1,\ldots,a_n]
    + \ldots + [a_1,a_2,\ldots,a_n-1],
  \end{equation*}
  which has the generating function
  \begin{equation*}
    M(x_1,x_2,\ldots,x_n)=\frac{1}{1-x_1-x_2-\ldots-x_n}.
  \end{equation*}

  \section{Stirling's numbers}
  \label{sec:stirlings-numbers}

  One reason for keeping the binomial coefficients with the same
  notation is that there is a striking parallel between it and what
  are called Stirling's numbers.  Define $S(n,k)$ and $C(n,k)$, where
  $S(n,k)$ is the number of ways to partition $n$ objects into $k$
  non-empty subsets, and where $C(n,k)$ is the number of elements in
  the symmetric group $S^n$ which have $k$ permutation
  cycles.\footnote{Knuth has better notation for these, but, with some
    amount of not-unnoticed irony, I'm not able to typeset them with
    my \ref[LaTeX-like typesetting software]{page_htmacros} which is
    used this website.  Anyway, Knuth describes these, which are known
    as Stirling's numbers of the second and first kind, respectively,
    much better than I do in \link[Two notes on
    notation]{http://arxiv.org/abs/math/9205211}.}  These (including
  $B$ for parallelism) have the following recurrence relations:
  \begin{equation*}
    B(n,k) = B(n-1,k) + B(n-1,k-1)
  \end{equation*}
  \begin{equation*}
    S(n,k) = kS(n-1,k) + S(n-1,k-1)
  \end{equation*}
  \begin{equation*}
    C(n,k) = (n-1)C(n-1,k) + C(n-1,k-1)
  \end{equation*}
  Each considers taking a single object from $n$ objects, and then
  handling two cases for the object:
  \begin{itemize}
  \item For $B$: whether it is not chosen or chosen to be among the
    $k$ chosen objects;
  \item For $S$: whether it is in one of the $k$ subsets with others
    or in its own singleton; or
  \item For $C$: whether it is in one of the cycles with others (in
    which case it can be between any of the $n-1$ objects in the
    permutation) or in its own 1-cycle.
  \end{itemize}

  Is there similar notation for $S$ and $C$ like $[a,b]$ for
  $B(a+b,b)$ which preserves the parallelism?  If so, can they be
  extended into a multi-$S$ and a multi-$C$?  If not, then it seems
  the parallelism between $B$ and the other two might be something of
  a red herring.

  Like how $[a,b]$ is $B(a+b,a)$, suppose we define
  $[a,b]_S=S(a+b,a)$.  This gives $[a,b]_s=a[a,b-1]_S+[a-1,b]_S$ using
  the above recurrence.  I'm far from convinced that this has given us
  anything.

  \section{Conclusion}
  \label{sec:conclusion}

  You can use deal with generating functions even if the recurrence
  relations is over multiple variables.  Going the other way, this
  suggests that polynomial long division can be done by first
  determining the corresponding recurrence relation.

  Also, the seemingly innocuous polynomial
  \begin{equation*}
    \frac{1}{1-x-y}
  \end{equation*}
  is hiding all of Pascal's triangle.
  
\end{page}

\begin{page}{intervals.html}
  \label{page_math_intervals}
  \title{Proving strict local maxima with interval arithmetic}
  \modified{9 Nov 2013}

  At least until recently, floating point numbers on a computer were
  to me approximations which I couldn't imagine trusting in any
  substantial way.  They're rounded!  You have nonsense like
  \texttt{1.0 + 1e-16 == 1.0}, or like \texttt{1.0/1 + 1.0/2 + 1.0/3 +
    ...} being a convergent series!  How could one obtain
  mathematically meaningful results?

  However, I learned recently that \link[IEEE 754 floating point
  numbers]{http://en.wikipedia.org/wiki/IEEE_floating_point} (the
  standard-issue floating point numbers on most hardware, sometimes
  called ``floats'') were designed to support obtaining mathematically
  meaningful results.  But, how can that be, given the nonsense above?
  These floating point numbers support doing what's called
  \link[interval
  arithmetic]{http://en.wikipedia.org/wiki/Interval_arithmetic}.\footnote{
    \link[Validated Numerics for
    Pedestrians]{http://www2.math.uu.se/~warwick/main/papers/ECM04Tucker.pdf}
    by Warwick Tucker is a short paper about the subject.}

  Basically, instead of representing a number as a single float you
  instead represent a closed interval which contains that number.  The
  basic operations (addition, multiplication, etc.) can be defined on
  intervals so that the bounds are set appropriately to give a
  minimally pessimistic interval which is guaranteed to contain the
  result.

  If we naively try to implement intervals, we'd get the same
  nonsense: \texttt{[1.0, 1.0] + [1e-16, 1e-16] == [1.0 + 1e-16, 1.0 +
    1e-16] == [1.0, 1.0]}, which is not an interval which contains the
  actual sum.  So, how is interval arithmetic possible on a computer?
  What does IEEE 754 give us?  The answer is rounding modes.  There
  are four rounding modes in IEEE 754: towards infinity, towards 0,
  towards negative infinity, and even-biased ``normal'' rounding (the
  nonsense-inducing-but-generally-what-you-want default).  Floating
  point hardware computes with more precision than is represented (but
  exactly as much as is required), and these rounding modes are for
  dealing with these extra bits.  So, the right thing to do with
  intervals is for the lower bounds to be computed rounding towards
  $-\infty$ and the upper bounds to be computed rounding towards
  $\infty$.  In the example above, the resulting interval would be
  \texttt{[1.0, 1.0000000000000002]}, which is, in fact, an interval
  which contains the result.

  Sometimes the resulting intervals can be too pessimistic.  One
  example is \texttt{x*x} if \texttt{x} is \texttt{[-1.0, 1.0]}.
  While we can tell that the minimal interval for \texttt{x*x} is
  \texttt{[0.0, 1.0]}, intervals don't know the correlation between
  the \texttt{x} on either side of the multiplication sign, so the
  resulting interval would be \texttt{[-1.0,
    1.0]}.\footnote{\link[Affine
    arithmetic]{http://en.wikipedia.org/wiki/Affine_arithmetic} is a
    way to preserve and make use of correlations.}  But, if we split
  the interval for \texttt{x} into the two intervals \texttt{[-1.0,
    0.0]} and \texttt{[0.0, 1.0]} and compute \texttt{x*x} on each, we
  can obtain the correct interval \texttt{[0.0, 1.0]} as the union of
  the images.

  So, what can these intervals help us do?  A first example is proving
  that a function $f$ has no roots on an interval \texttt{[a, b]},
  where $f$ is a function composed of interval-friendly operations.
  If we compute \texttt{$f$([a, b])} and see that \texttt{0.0} is not
  in the resulting interval, then this is a \textit{proof} that $f$
  has no root there.  Otherwise, while it might be that \texttt{0.0}
  appears in the interval, because intervals are pessimistic, this
  does not constitute a proof that $f$ has a root (consider
  \texttt{x*x + 1} on \texttt{[-2.0, 2.0]}).  But, we might be able to
  subdivide that interval and show that for each subinterval that
  \texttt{0.0} is not present.  Each of these individual proofs would,
  together, constitute a proof of $f$ having no root anywhere on
  \texttt{[a, b]}.

  Suppose we have a function $f$, its derivative $f'$, and its second
  derivative $f''$ (with each of these continuous).  If we want to
  prove that $f$ has a strict local maximum in \texttt{[a, b]}, we can
  try the following process:
  \begin{enumerate}
  \item Show that $f'$ has a root on that interval using the
    intermediate value theorem (possibly using $f'$ and $f''$ with
    Newton's method).
  \item Show that $f''$ has no root, using the process outlined above.
  \item Show that $f''$ is negative somewhere on that interval.
  \end{enumerate}
  With these together, we can conclude that $f$ must reach a local
  maximum on that interval, and, furthermore, because the second
  derivative is never zero, that $f'$ is injective on that interval,
  and so it has exactly one root.

  Later, I will provide code to compute this as well as show how to
  use automatic differentiation techniques to compute \textit{exact}
  derivatives.  With these techniques, I'm able to prove that $x^3-x$
  has a strict local maximum between $-2$ and $-0.1$ as well as prove that
  \begin{equation*}
    \frac{1}{1+x^2}
  \end{equation*}
  has a strict local maximum between $-0.1$ and $0.1$.

\end{page}

\begin{page}{roots_unity.html}
  \label{page_math_roots_unity}
  \title{Sums of roots of unity}
  \modified{3 Jan 2014}

  \begin{figure}[r]
    \label{fig_roots_unity}
    \includegraphics[alt=Ninth roots of unity,width=242]{roots_unity_9.png}
    \begin{center}
      \caption{Ninth roots of unity on the complex plane.}
    \end{center}
  \end{figure}

  This note, which I'll probably expand upon later, is about roots of
  unity.  I was reading Shafarevich's excellent
  \link[\textit{Discourses on
    Algebra}]{http://www.amazon.com/Discourses-Algebra-Igor-R-Shafarevich/dp/3540422536},
  and wondered what the identity at the end of section 2.4 meant for
  the polynomial $x^n-1$, the defining polynomial for the $n$th roots
  of unity (where $n>1$).

  The reason I wondered was that the roots of unity are very
  symmetric: they lie evenly along the unit circle in the complex
  plane.  As an example, the ninth roots of unity (i.e., the roots of
  the polynomial when $n$ is $9$) are represented in figure
  \ref{#fig_roots_unity}.

  Really, I started thinking about the identity (as \ref[discussed
  below]{#sec:shafarevich}), but then I wondered what other ways there
  are to show that the sum of all of the roots of unity is $0$.  This
  note has a number of ways of showing this fact.

  \section{The intuitive way}
  \label{sec:intuitive-way}

  The $n$th roots of unity lie evenly on the unit circle, so their
  center of mass better be at the origin.  So, the sum of the complex
  numbers as vectors is zero.

  \section{The direct way}
  \label{sec:direct-way}

  The most direct way to find the sum of the $n$th roots of unity is
  as follows.  Let $x=\omega_0+\cdots+\omega_{n-1}$ be the sum of all
  $n$ of the roots of unity.  Since roots of unity have unit length,
  since $\omega_i^n=1$ for all $i$, and since $1/\omega_i$ is also an
  $n$th root of unity, $\omega_ix$ must be exactly $x$ ($\omega_i$
  just permutes the components of the sum).  That is,
  $(1-\omega_i)x=0$, which means either $x=0$ or $\omega_i=1$, and
  because we can choose an $i$ such that $\omega_i\neq1$, we conclude
  that $x=0$.

  \section{By representation theory}
  \label{sec:repr-theory}

  The direct way reminded me of one-dimensional representations of the
  cyclic group $C_n$.  Take an irreducible complex one-dimensional
  representation $\rho$ of $C_n=\gen{\alpha}$, and first note that the
  value of $\rho_\alpha$ completely characterizes $\rho$ since
  $\rho_{\alpha^k}=(\rho_\alpha)^k$.

  One can show that all of the irreducible representations of $C_n$
  are one-dimensional from the fact that $C_n$ is abelian, and
  furthermore, using the fact that the sum of the squares of the
  dimensions of the irreducible representations is equal to $|C_n|$,
  we can conclude that each root of unity corresponds to a different
  irreducible representation of $C_n$.

  Next, we compute $x=1+\alpha+\alpha^2+\cdots+\alpha^{n-1}$.  Since
  $\rho_x$ is an endomorphism of complex representations from
  irreducible $\rho$, it must be some constant in $\C$.  We can see,
  as before, that $1-\alpha$ is in the kernel of $\rho_x$, so either
  $\rho$ is the trivial representation or $x=0$, and since we can
  choose $\rho$ not to be trivial, this completes the proof.

  \section{By polynomial theory}
  \label{sec:poly-theory}

  This kind of method should be well known to anyone who did
  high-school competition math.  We take the polynomials $f(x)=x^n-1$
  and $g(x)=(x-\omega_0)(x-\omega_1)\cdots(x-\omega_{n-1})$ and note
  that they are the same because they are both degree $n$, have the
  same $n$ roots, and are monic.  We use the fact they are monic by
  seeing that dividing each polynomial through by $x-\omega_i$ results
  in a monic polynomial, and so dividing through by all such monomials
  will result in 1 for each of them, proving $f=g$.  Now, the
  $x^{n-1}$ term has $0$ as a coefficient according to $f$ and
  $-\omega_0-\omega_1-\cdots-\omega_{n-1}$ according to $g$, which
  completes the proof.

  \section{By Shafarevich's identity}
  \label{sec:shafarevich}

  This method is the reason I thought about any of this to begin with.
  It's not very direct, but I think it is kind of interesting.  First,
  we need the identity from the end of section 2.4 of
  \textit{Discourses on Algebra}, which I will derive here.  To do
  this, we will develop the theory of the interpolating polynomial.
  The idea is that, given a set of $n+1$ values $v_i$ at distinct
  values $x_i$, what is the $n$th degree polynomial $f$ with
  $f(x_i)=v_i$ for all $i$?

  Let $F(x)=(x-x_1)(x-x_2)\cdots(x-x_{n+1})$, and let $F_i(x)$ be the
  polynomial obtained by dividing $F$ through by $x-x_i$.  We define
  \begin{equation*}
    f_i(x)=\frac{1}{F_i(x_i)}F_i(x)
  \end{equation*}
  so that $f_i(x_i)=1$ and $f_i(x_j)=0$ when $i\neq j$.  This provides
  a convenient basis for polynomials: observe that
  \begin{equation*}
    f(x)=v_1f_1(x) + v_2f_2(x) + \cdots + v_{n+1}f_{n+1}(x)   
  \end{equation*}
  is the interpolating polynomial as discussed.

  If $k$ is a natural number not exceeding $n$, if we have
  $v_i=x_i^k$, then since $f(x)$ must be $x^k$, we have
  \begin{equation*}
    x^k = \frac{x_1^k}{F_1(x_1)}F_1(x) + \cdots + \frac{x_{n+1}^k}{F_{n+1}(x_{n+1})}F_{n+1}(x)
  \end{equation*}
  and since each $F_i$ is a monic degree-$n$ polynomial, this means
  that if $k < n$,
  \begin{equation*}
    \frac{x_1^k}{F_1(x_1)} + \cdots + \frac{x_{n+1}^k}{F_{n+1}(x_{n+1})} = 0,
  \end{equation*}
  and if $k = n$, then 
  \begin{equation*}
    \frac{x_1^n}{F_1(x_1)} + \cdots + \frac{x_{n+1}^n}{F_{n+1}(x_{n+1})} = 1.
  \end{equation*}
  These are the identities at the end of section 2.4.

  We won't be using these identities directly, but rather in
  spirit\footnote{Truthfully, this is because I forgot the exact
    formulation of the identities while considering their application
    to roots of unity.}.  We let $x_i=\omega_i$ and $v_i=1$, where
  $\omega_i$ is an $n$th root of unity (with $\omega_1=1$).  Notice
  that each $F_i$ is an $(n-1)$-degree polynomial, so since both
  $f(x)$ and $g(x)=1$ are at-most-$(n-1)$-degree polynomials which
  agree at $n$ points, $f=g$.  By taking the leading $x^{n-1}$
  coefficient, this means
  \begin{equation*}
    \frac{1}{F_1(\omega_1)} + \cdots + \frac{1}{F_n(\omega_n)} = 0.
  \end{equation*}

  Notice that $F_i(\omega_i)$ is the product of all $\omega_i -
  \omega_j$ for $j\neq i$, and
  $\omega_i-\omega_j=\omega_i(1-\omega_i\inv\omega_j)$.  Then, we can
  see $F_i(\omega_i) = \omega_i^{n-1}F_1(\omega_1)$ since
  $\omega_i\inv$ is just permuting the roots of unity.  Because
  $(\omega_i^{n-1})\inv=\omega_i$, we have
  \begin{equation*}
    \frac{\omega_1}{F_1(\omega_1)} + \cdots + \frac{\omega_n}{F_1(\omega_1)} = 0,
  \end{equation*}
  and so $\omega_1+\cdots+\omega_n = 0$.
  
\end{page}

\begin{page}{fib_diff_eq.html}
  \label{page_math_fib_diffeq}
  \title{The Fibonacci sequence by way of differential equations}
  \modified{29 April 2016}

  The well-known Fibonacci sequence,
  \begin{equation*}
    0,1,1,2,3,5,8,\ldots,
  \end{equation*}
  is described by the recurrence relation $c_{n+2}=c_n+c_{n+1}$, with
  the initial conditions $c_0=0$ and $c_1=1$.  An interesting thing
  you can do is to create what is called a generating function
  \begin{equation*}
    f(t)=\Sigma\,c_n t^n
  \end{equation*}
  (where the sum is over $0\leq n$) which is a power series that holds
  onto a sequence as coefficients (and, basically, linear independence
  of $1,t,t^2,\ldots$ means they are recoverable).  For the Fibonacci
  sequence, this is
  \begin{equation*}
    f(t)=0+t+t^2+2t^3+3t^4+5t^5+8t^6+\cdots
  \end{equation*}
  The great thing about writing a sequence in this way is that
  multiplying the generating function by $t$ shifts it right-ward,
  inserting a $0$ at the beginning.  So, we can make use of the fact
  that each term is the sum of the preceding two to get a simpler
  characterization of $f$:
  \begin{equation*}
    (1-t-t^2)f(t)=t
  \end{equation*}
  which of course means $f(t)=t/(1-t-t^2)$.  This, by the way, gives
  rational numbers you can use to amaze and entertain your
  mathematically-literate friends.  For instance,
  $100/9899\approx0.01010203050813\ldots$.  But what you can also do
  is use the partial fraction decomposition of $t/(1-t-t^2)$
  \begin{equation*}
    f(t)=5^{-1/2}((1-\phi t)^{-1} - (1-(1-\phi)t)^{-1})
  \end{equation*}
  where $\phi$ is the golden ratio, $(1+5^{1/2})/2$, and use the power
  series expansions of these reciprocals to obtain a closed form
  equation for the Fibonacci sequence, namely
  \begin{equation*}
    c_n = 5^{-1/2}(\phi^n - (1-\phi)^n).
  \end{equation*}

  While leading recitation for a differential equations course, I
  realized it's possible to get this same equation by way of
  differential equations.  What we start with is the homogenous
  second-order linear differential equation $y''-y'-y=0$, and solve it
  the normal way (using the characteristic equation), by finding a
  Taylor series, and then comparing the two solutions.  In contrast to
  above, where the sequence is stored as coefficients to a power
  series, the sequence will instead be stored as the derivatives of
  some function.  The characteristic polynomial $r^2-r-1$ has roots
  $\phi$ and $(1-\phi)$, so every solution is of the form
  \begin{equation*}
    y=Ae^{\phi t}+Be^{(1-\phi)t}
  \end{equation*}
  for some constants $A$ and $B$.  Because I know where this is going,
  let's solve the initial value problem with $y(0)=0$ and $y'(0)=1$,
  which (sparing you the details) gives
  \begin{equation*}
    y=5^{-1/2}(e^{\phi t}-e^{(1-\phi)t}).
  \end{equation*}
  This readily gives us a Taylor series:
  \begin{equation*}
    y=\Sigma 5^{-1/2}(\phi^n-(1-\phi)^n) t^n/n!.
  \end{equation*}
  But we can also solve such differential equations as Taylor series
  directly.  Supposing the solution is of the form
  $y=\Sigma c_n t^n/n!$ (and every solution indeed has a Taylor series
  due to theory), we can differentiate this to get
  $y'=\Sigma c_{n+1}t^n/n!$ and $y''=\Sigma c_{n+2}t^n/n!$, and so a
  solution must satisfy $\Sigma (c_{n+2}-c_{n+1}-c_n)t^n/n!=0$.  That
  is, the coefficients (excepting the $n!$'s) satisfy a linear
  recurrence relation $c_{n+2}=c_{n}+c_{n+1}$, sort of like the power
  series case.  Every such solution is entirely determined by the
  values of $c_0$ and $c_1$, and these correspond to $y(0)$ and
  $y'(0)$, respectively.  This means these coefficients form the
  Fibonacci sequence, and by comparing with the previous solution to
  the differential equation, we once obtain obtain the following
  closed-form formula
  \begin{equation*}
    c_n=5^{-1/2}(\phi^n-(1-\phi)^n).
  \end{equation*}

\end{page}